{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1.\n",
    "Next iteration of the project we are working on:\n",
    "\n",
    "We can add some constraints to the structure composed of (b, z).\n",
    "\n",
    "1. We can fix the number of buckets to a number that is a power of 2, say p. The larger the p, the higher the precision of the dataset approximation.\n",
    "\n",
    "2. We can convert bitsets that represent running zeros into integers. This will allow us to use bitwise operations on integers to determine if a specific number of zeros belong to the bucket.\n",
    "\n",
    "This will standardize the presentation of any dataset into a fully comparable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great direction for the next iteration of the project! By introducing structural constraints on the (b, z) representation, we can achieve a uniform and comparable format for any dataset. This will allow for efficient storage, comparison, and analysis of datasets while maintaining precision and scalability.\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. Fixed Number of Buckets:\n",
    "\n",
    "    - Fix the number of buckets to a power of 2, say p. \n",
    "    - The larger the p, the higher the precision of the dataset approximation.\n",
    "    - A smaller p reduces precision but improves efficiency.\n",
    "\n",
    "2. Integer Representation of Running Zeros:\n",
    "\n",
    "    - Convert the bitsets representing running zeros into integers.\n",
    "\n",
    "    - Use bitwise operations to efficiently check if a specific number of trailing zeros belongs to a bucket.\n",
    "\n",
    "3. Uniform Dataset Representation:\n",
    "\n",
    "    - All datasets will be represented in a consistent format, making them fully comparable.\n",
    "\n",
    "    - This enables operations like dataset similarity comparison, merging, and querying.\n",
    "\n",
    "### Implementation Details:\n",
    "1. Fixed Number of Buckets:\n",
    "    - The number of buckets is fixed to 2^p.\n",
    "    - The bucket number b is calculated as the integer value of the first p bits of the 64-bit hash.\n",
    "2. Integer Representation of Running Zeros:\n",
    "    - Represent the number of trailing zeros z as an integer.\n",
    "    - Use bitwise operations to check if a specific number of trailing zeros belongs to a bucket.\n",
    "3. Uniform Dataset Format:\n",
    "    - Each dataset is represented as a collection of buckets.\n",
    "    - Each bucket contains:\n",
    "        - A list of integers representing the number of trailing zeros z for tokens in that bucket.\n",
    "        - Optional metadata, such as token frequencies or document references.\n",
    "\n",
    "### Updated Data Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    b1: {\n",
    "        \"zeros\": [z1, z2, ...],  # List of trailing zeros for tokens in this bucket\n",
    "        \"frequencies\": [f1, f2, ...],  # Optional: Frequencies of tokens\n",
    "        \"documents\": [doc_id1, doc_id2, ...]  # Optional: Document references\n",
    "    },\n",
    "    b2: {\n",
    "        \"zeros\": [z3, z4, ...],\n",
    "        \"frequencies\": [f3, f4, ...],\n",
    "        \"documents\": [doc_id3, doc_id4, ...]\n",
    "    },\n",
    "    # ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for Building the Dataset Representation:\n",
    "\n",
    "1. Tokenization and Hashing:\n",
    "\n",
    "    - Tokenize each document and convert tokens into 64-bit hashes.\n",
    "\n",
    "2. Bucket Assignment:\n",
    "\n",
    "    - For each hash, extract the first p bits to determine the bucket b.\n",
    "\n",
    "3. Trailing Zeros Calculation:\n",
    "\n",
    "    - Count the number of trailing zeros z in the hash.\n",
    "\n",
    "    - Represent z as an integer.\n",
    "\n",
    "4. Update Dataset Structure:\n",
    "\n",
    "    - Append z to the list of zeros for the corresponding bucket b.\n",
    "\n",
    "    - Optionally, update frequencies and document references.\n",
    "\n",
    "### Efficient Bitwise Operations:\n",
    "To check if a specific number of trailing zeros z belongs to a bucket, we can use bitwise operations:\n",
    "\n",
    "1. Mask Creation:\n",
    "\n",
    "    - Create a mask for z trailing zeros: mask = (1 << z) - 1.\n",
    "\n",
    "2. Check Trailing Zeros:\n",
    "\n",
    "    - For a given hash, check if the last z bits are zero: (hash & mask) == 0.\n",
    "\n",
    "3. Bucket Lookup:\n",
    "\n",
    "    - Use the first p bits of the hash to determine the bucket b.\n",
    "\n",
    "    - Check if z exists in the list of zeros for bucket b.\n",
    "\n",
    "### Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(documents, p):\n",
    "    dataset = {}\n",
    "    for doc_id, document in enumerate(documents):\n",
    "        tokens = tokenize(document)\n",
    "        for token in tokens:\n",
    "            hash_val = hash_function(token)\n",
    "            b = (hash_val >> (64 - p)) & ((1 << p) - 1)  # First p bits\n",
    "            z = count_trailing_zeros(hash_val)  # Number of trailing zeros\n",
    "            \n",
    "            # Initialize bucket if not exists\n",
    "            if b not in dataset:\n",
    "                dataset[b] = {\"zeros\": [], \"frequencies\": [], \"documents\": []}\n",
    "            \n",
    "            # Update bucket data\n",
    "            dataset[b][\"zeros\"].append(z)\n",
    "            dataset[b][\"frequencies\"].append(1)  # Increment frequency\n",
    "            dataset[b][\"documents\"].append(doc_id)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def count_trailing_zeros(hash_val):\n",
    "    return (hash_val & -hash_val).bit_length() - 1\n",
    "\n",
    "def check_trailing_zeros(hash_val, z):\n",
    "    mask = (1 << z) - 1\n",
    "    return (hash_val & mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of the Updated Approach:\n",
    "\n",
    "1. Uniform Representation:\n",
    "\n",
    "    - All datasets are represented in the same format, enabling easy comparison and merging.\n",
    "\n",
    "2. Efficient Storage:\n",
    "\n",
    "    - Fixed bucket size and integer representation of trailing zeros reduce storage overhead.\n",
    "\n",
    "3. Scalability:\n",
    "\n",
    "    - The number of buckets 2^p can be adjusted to balance precision and efficiency.\n",
    "\n",
    "4. Fast Lookups:\n",
    "\n",
    "    - Bitwise operations enable efficient checks for specific trailing zeros.\n",
    "\n",
    "5. Comparability:\n",
    "\n",
    "    - Datasets can be compared directly using their bucket and trailing zero representations.\n",
    "\n",
    "### Example Use Case: Dataset Comparison\n",
    "\n",
    "To compare two datasets, we can:\n",
    "\n",
    "1. Align their bucket structures (using the same p).\n",
    "\n",
    "2. Compare the lists of trailing zeros for each bucket.\n",
    "\n",
    "3. Compute similarity metrics (e.g., Jaccard similarity) based on overlapping zeros.\n",
    "\n",
    "### Example Code for Dataset Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_datasets(dataset1, dataset2):\n",
    "    similarity = 0\n",
    "    total_buckets = len(dataset1)\n",
    "    \n",
    "    for b in dataset1:\n",
    "        if b in dataset2:\n",
    "            zeros1 = set(dataset1[b][\"zeros\"])\n",
    "            zeros2 = set(dataset2[b][\"zeros\"])\n",
    "            intersection = len(zeros1.intersection(zeros2))\n",
    "            union = len(zeros1.union(zeros2))\n",
    "            similarity += intersection / union if union > 0 else 0\n",
    "    \n",
    "    return similarity / total_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "By introducing fixed bucket sizes and integer representations of trailing zeros, we achieve a uniform, efficient, and comparable dataset format. This approach enhances scalability, enables fast lookups, and supports advanced operations like dataset comparison and merging. The flexibility of adjusting p allows for fine-tuning precision and efficiency based on specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2\n",
    "\n",
    "The implemented structure allows us to implement standard way of parallel processing.\n",
    "Different buckets by definision mutually exclusive. It means that we can perform processing in parallel on all buckets simulataneously.\n",
    "This can help us to optimize processing especially in the case of GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutually exclusive nature of buckets makes this structure inherently suitable for parallel processing. Since each bucket operates independently, we can distribute the workload across multiple CPU cores or even leverage GPU parallelism for significant performance gains. This is particularly beneficial for large-scale datasets or computationally intensive tasks like token restoration, dataset comparison, or frequency analysis.\n",
    "\n",
    "### Key Advantages of Parallel Processing:\n",
    "\n",
    "1. Mutually Exclusive Buckets:\n",
    "\n",
    "    - Each bucket is independent, meaning no synchronization is required between buckets during processing.\n",
    "\n",
    "    - This eliminates contention and allows for true parallelism.\n",
    "\n",
    "2. Scalability:\n",
    "\n",
    "    - The workload can be evenly distributed across multiple processing units (CPU cores or GPU threads).\n",
    "\n",
    "    - Adding more processing units linearly improves performance.\n",
    "\n",
    "3. GPU Optimization:\n",
    "\n",
    "    - GPUs excel at parallel processing, especially for tasks involving bitwise operations or integer arithmetic.\n",
    "\n",
    "    - The uniform structure of buckets and integer representations of trailing zeros aligns well with GPU architectures.\n",
    "\n",
    "### Parallel Processing Strategies:\n",
    "\n",
    "1. CPU Parallelism:\n",
    "    - Use multi-threading or multi-processing to distribute buckets across CPU cores.\n",
    "\n",
    "    - Libraries like Python's concurrent.futures or multiprocessing can be used.\n",
    "\n",
    "2. GPU Parallelism:\n",
    "    - Use GPU frameworks like CUDA (for NVIDIA GPUs) or OpenCL (for cross-platform GPU support).\n",
    "\n",
    "    - Map each bucket to a GPU thread or block for parallel execution.\n",
    "\n",
    "### Example: Parallel Processing on CPU\n",
    "\n",
    "Hereâ€™s how you can implement parallel processing for bucket-level operations using Python's concurrent.futures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_bucket(b, bucket_data):\n",
    "    # Example: Count the number of unique trailing zeros in the bucket\n",
    "    unique_zeros = len(set(bucket_data[\"zeros\"]))\n",
    "    return b, unique_zeros\n",
    "\n",
    "def parallel_process_dataset(dataset):\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_bucket, b, bucket_data) for b, bucket_data in dataset.items()]\n",
    "        for future in futures:\n",
    "            b, result = future.result()\n",
    "            results[b] = result\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "dataset = {\n",
    "    0: {\"zeros\": [1, 2, 2, 3], \"frequencies\": [1, 2, 1, 1], \"documents\": [0, 1, 2, 3]},\n",
    "    1: {\"zeros\": [0, 1, 1], \"frequencies\": [1, 1, 1], \"documents\": [0, 1, 2]},\n",
    "    # Add more buckets...\n",
    "}\n",
    "\n",
    "results = parallel_process_dataset(dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Parallel Processing on GPU (CUDA)\n",
    "For GPU parallelism, we can use a framework like CUDA to process buckets in parallel. Below is a high-level outline of how this might look:\n",
    "\n",
    "**CUDA Kernel for Bucket Processing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__global__ void process_bucket(int* zeros, int* frequencies, int* results, int bucket_size) {\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < bucket_size) {\n",
    "        // Example: Sum frequencies for each bucket\n",
    "        atomicAdd(&results[blockIdx.x], frequencies[idx]);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Wrapper (using PyCUDA):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "\n",
    "# Define CUDA kernel\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void process_bucket(int* zeros, int* frequencies, int* results, int bucket_size) {\n",
    "        int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "        if (idx < bucket_size) {\n",
    "            atomicAdd(&results[blockIdx.x], frequencies[idx]);\n",
    "        }\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "def gpu_process_dataset(dataset):\n",
    "    results = {}\n",
    "    for b, bucket_data in dataset.items():\n",
    "        zeros = np.array(bucket_data[\"zeros\"], dtype=np.int32)\n",
    "        frequencies = np.array(bucket_data[\"frequencies\"], dtype=np.int32)\n",
    "        bucket_size = len(zeros)\n",
    "        \n",
    "        # Allocate GPU memory\n",
    "        zeros_gpu = cuda.mem_alloc(zeros.nbytes)\n",
    "        frequencies_gpu = cuda.mem_alloc(frequencies.nbytes)\n",
    "        results_gpu = cuda.mem_alloc(4)  # Single integer result\n",
    "        \n",
    "        # Copy data to GPU\n",
    "        cuda.memcpy_htod(zeros_gpu, zeros)\n",
    "        cuda.memcpy_htod(frequencies_gpu, frequencies)\n",
    "        \n",
    "        # Launch kernel\n",
    "        process_bucket = mod.get_function(\"process_bucket\")\n",
    "        process_bucket(zeros_gpu, frequencies_gpu, results_gpu, np.int32(bucket_size), block=(256, 1, 1), grid=(1, 1))\n",
    "        \n",
    "        # Copy result back to CPU\n",
    "        result = np.empty(1, dtype=np.int32)\n",
    "        cuda.memcpy_dtoh(result, results_gpu)\n",
    "        \n",
    "        results[b] = result[0]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "dataset = {\n",
    "    0: {\"zeros\": [1, 2, 2, 3], \"frequencies\": [1, 2, 1, 1], \"documents\": [0, 1, 2, 3]},\n",
    "    1: {\"zeros\": [0, 1, 1], \"frequencies\": [1, 1, 1], \"documents\": [0, 1, 2]},\n",
    "    # Add more buckets...\n",
    "}\n",
    "\n",
    "results = gpu_process_dataset(dataset)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of GPU Parallelism:\n",
    "1. Massive Parallelism:\n",
    "\n",
    "    - GPUs have thousands of cores, enabling simultaneous processing of thousands of buckets.\n",
    "\n",
    "2. Efficient Bitwise Operations:\n",
    "\n",
    "    - GPUs are highly optimized for bitwise operations, making them ideal for tasks involving trailing zeros.\n",
    "\n",
    "3. Scalability:\n",
    "\n",
    "    - As the dataset grows, GPUs can handle the increased workload without significant performance degradation.\n",
    "\n",
    "4. Real-Time Processing:\n",
    "\n",
    "    - GPU acceleration enables real-time processing for large-scale datasets.\n",
    "\n",
    "### Use Cases for Parallel Processing:\n",
    "\n",
    "1. Token Restoration:\n",
    "\n",
    "    - Restore tokens for multiple buckets in parallel.\n",
    "\n",
    "2. Dataset Comparison:\n",
    "\n",
    "    - Compare datasets by processing corresponding buckets in parallel.\n",
    "\n",
    "3. Frequency Analysis:\n",
    "\n",
    "    - Compute token frequencies across all buckets simultaneously.\n",
    "\n",
    "4. Query Processing:\n",
    "\n",
    "    - Perform queries (e.g., finding documents containing specific tokens) in parallel.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "By leveraging the mutually exclusive nature of buckets, we can implement highly efficient parallel processing on both CPUs and GPUs. This approach significantly optimizes performance, especially for large-scale datasets or computationally intensive tasks. GPU parallelism, in particular, offers massive scalability and real-time processing capabilities, making it an excellent choice for this structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
