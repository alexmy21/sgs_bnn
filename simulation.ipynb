{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGS simulation using Enron emails as a source. Day-by-day processing\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "# Pkg.activate(\".\")\n",
    "# Pkg.instantiate()\n",
    "\n",
    "# Pkg.add(\"PyCall\")\n",
    "# Pkg.add(\"DataFrames\")\n",
    "# Pkg.add(\"CSV\")\n",
    "# Pkg.add(\"Plots\")\n",
    "# Pkg.add(\"StatsPlots\")\n",
    "# Pkg.add(\"StatsBase\")\n",
    "# Pkg.add(\"GLM\")\n",
    "# Pkg.add(\"StatsModels\")\n",
    "# Pkg.add(\"Dates\")\n",
    "# Pkg.add(\"CSV\")\n",
    "# Pkg.add(\"EasyConfig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"src/sgs_store.jl\")\n",
    "\n",
    "# Import necessary packages\n",
    "using PyCall\n",
    "using DataFrames\n",
    "# import polars as pl\n",
    "using Dates\n",
    "using CSV\n",
    "\n",
    "# Import the fine_tune_model and parse_decoded_strings functions from the Python script\n",
    "py\"\"\"\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from SGS_Transformers import BertTokenizerWrapper, RobertaTokenizerWrapper, GPT2TokenizerWrapper\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis   = pyimport(\"redis\")\n",
    "\n",
    "# Define a function to initialize the tokenizer\n",
    "function initialize_tokenizer()\n",
    "    return py\"RobertaTokenizerWrapper\"()\n",
    "end\n",
    "\n",
    "# Define a function to filter the DataFrame by date\n",
    "function filter_dataframe_by_date(df, date)\n",
    "    return df[df.Date .== date, :]\n",
    "end\n",
    "\n",
    "# Define a function to process each column\n",
    "function process_column(r, tokenizer, filtered_df, column, _parent, chunk_size)\n",
    "    col_values  = filtered_df[:, column]\n",
    "    col_sha1    = Util.sha1_union([_parent, string(column)])\n",
    "    column_size = Base.summarysize(col_values)\n",
    "    num_chunks  = ceil(Int, column_size / chunk_size)\n",
    "    chunks      = Store.chunk_array(col_values, num_chunks)\n",
    "\n",
    "    println(col_sha1, \"; num_chunks: \", num_chunks)\n",
    "    dataset = Store.ingest_df_column(r, tokenizer, chunks, col_sha1)\n",
    "    dataset_vector = Vector{UInt32}(dataset)\n",
    "\n",
    "    hll = HllSets.HllSet{10}()\n",
    "    _hll = HllSets.restore!(hll, dataset_vector)\n",
    "    \n",
    "    println(\"hll: \", HllSets.id(_hll), \"; \", HllSets.count(_hll))\n",
    "    \n",
    "    entity = Entity.Instance{10}(r, _hll, prefix=\"b:col\")\n",
    "    \n",
    "    return entity\n",
    "end\n",
    "\n",
    "# Define a function to process each column\n",
    "function process_row(r, tokenizer, filtered_df, _parent)\n",
    "    col_sha1    = Util.sha1_union([_parent])\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for row in eachrow(filtered_df)\n",
    "        # Join column values by space\n",
    "        row_str = join(row, \" \")\n",
    "        push!(rows, row_str)\n",
    "    end\n",
    "        \n",
    "    dataset = Store.ingest_df_rows(r, tokenizer, rows, col_sha1)\n",
    "    # dataset_vector = Vector{UInt32}(dataset)\n",
    "\n",
    "    hll = HllSets.HllSet{10}()\n",
    "    # _hll = HllSets.restore!(hll, dataset_vector)\n",
    "    \n",
    "    # println(\"hll: \", HllSets.id(_hll), \"; \", HllSets.count(_hll))\n",
    "    \n",
    "    entity = Entity.Instance{10}(r, hll, prefix=\"b:row\")\n",
    "    \n",
    "    return entity\n",
    "end\n",
    "\n",
    "# Define a function to process the DataFrame\n",
    "function process_dataframe(r, start, tokenizer, df, dates_vector, cols, _parent, chunk_size, threshold, batch)\n",
    "    i = start\n",
    "    while true && i < length(dates_vector)\n",
    "        the_date = dates_vector[i]\n",
    "        filtered_df = filter_dataframe_by_date(df, the_date)\n",
    "\n",
    "        for column in cols\n",
    "            entity = process_column(r, tokenizer, filtered_df, column, _parent, chunk_size)\n",
    "            println(\"Current Date:\", the_date)\n",
    "        end\n",
    "\n",
    "        process_row(r, tokenizer, filtered_df, _parent)\n",
    "\n",
    "        i += 1\n",
    "        # println(\"i = \", i)\n",
    "        if i > threshold\n",
    "            threshold += batch\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return i, threshold\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main function to run the demo\n",
    "function main(csv_file_path::String, start, chunk_size, threshold, batch)\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = initialize_tokenizer()\n",
    "\n",
    "    # Define other necessary variables\n",
    "    r = redis.Redis(host=\"localhost\", port=6379, db=0)  # Redis connection or other necessary setup\n",
    "    df = DataFrame(CSV.File(csv_file_path, header=true, select=[:Date, :From, :To, :Subject, :content, :user]))\n",
    "\n",
    "    # Reformat fields :Date, f:From, and :To\n",
    "    df.Date = map(x -> Dates.format(Dates.DateTime(x, \"yyyy-mm-dd HH:MM:SS\"), \"yyyy-mm-dd\"), df.Date)\n",
    "    df.From = map(x -> ismissing(x) ? \"\" : (isnothing(match(r\"'([^']*)'\", x)) ? \"\" : match(r\"'([^']*)'\", x).captures[1]), df.From)\n",
    "    df.To   = map(x -> ismissing(x) ? \"\" : (isnothing(match(r\"'([^']*)'\", x)) ? \"\" : match(r\"'([^']*)'\", x).captures[1]), df.To)\n",
    "    \n",
    "    # Extract distinct dates from the Date column, order them in ascending order, and convert to a vector\n",
    "    distinct_dates  = unique(df.Date)\n",
    "    sorted_dates    = sort(distinct_dates)    \n",
    "    dates_vector    = collect(sorted_dates)\n",
    "\n",
    "    cols        = [:From, :To, :Subject, :content, :user]\n",
    "    _parent     = csv_file_path\n",
    "    chunk_size  = chunk_size\n",
    "    threshold   = threshold\n",
    "    batch       = batch\n",
    "\n",
    "    # Process the DataFrame\n",
    "    return process_dataframe(r, start, tokenizer, df, dates_vector, cols, _parent, chunk_size, threshold, batch)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/home/alexmy/Downloads/POC/DATA/enron_05_17_2015_with_labels_v2.csv\"\n",
    "chunk_size = 512000\n",
    "threshold = 10\n",
    "batch = 10\n",
    "start = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, threshold = main(csv_file_path, start, chunk_size, threshold, batch)\n",
    "start, threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
